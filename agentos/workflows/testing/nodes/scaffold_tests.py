"""N2: Scaffold Tests node for TDD Testing Workflow.

Generates executable test stubs from the approved test plan:
- Each test has `assert False, "TDD: Implementation pending"`
- Tests are syntactically valid and RUNNABLE
- Uses pytest conventions and fixtures
"""

import re
from pathlib import Path
from typing import Any

from agentos.workflows.testing.audit import (
    get_repo_root,
    log_workflow_execution,
    next_file_number,
    save_audit_file,
)
from agentos.workflows.testing.knowledge.patterns import get_test_type_info
from agentos.workflows.testing.state import TestingWorkflowState, TestScenario


def _extract_impl_module(files_to_modify: list[dict] | None) -> str | None:
    """Extract Python module path from files_to_modify.

    Finds the first Python file that will be created/modified and converts
    its path to a Python import path.

    Args:
        files_to_modify: List of file dicts from LLD Section 2.1.

    Returns:
        Python module path (e.g., 'agentos.workflows.testing.nodes.foo')
        or None if no Python files found.
    """
    if not files_to_modify:
        return None

    for file_info in files_to_modify:
        path = file_info.get("path", "")
        change_type = file_info.get("change_type", "").lower()

        # Skip test files - we want implementation modules
        if "test" in path.lower():
            continue

        # Look for Python files being added or modified
        if path.endswith(".py") and change_type in ("add", "modify"):
            # Convert file path to module path
            # e.g., "agentos/workflows/testing/nodes/foo.py" -> "agentos.workflows.testing.nodes.foo"
            module = path.replace("/", ".").replace("\\", ".")
            if module.endswith(".py"):
                module = module[:-3]
            # Remove src/ prefix if present
            if module.startswith("src."):
                module = module[4:]
            return module

    return None


def generate_test_file_content(
    scenarios: list[TestScenario],
    module_name: str,
    issue_number: int,
    files_to_modify: list[dict] | None = None,
) -> str:
    """Generate pytest file content from test scenarios.

    Args:
        scenarios: List of test scenarios.
        module_name: Name of the module being tested.
        issue_number: GitHub issue number.
        files_to_modify: List of files from LLD Section 2.1 (for import paths).

    Returns:
        Python test file content.
    """
    # Group scenarios by test type
    unit_tests = [s for s in scenarios if s.get("test_type") == "unit"]
    integration_tests = [s for s in scenarios if s.get("test_type") == "integration"]
    e2e_tests = [s for s in scenarios if s.get("test_type") == "e2e"]
    other_tests = [
        s for s in scenarios
        if s.get("test_type") not in ("unit", "integration", "e2e")
    ]

    # Extract module import path from files_to_modify
    impl_module = _extract_impl_module(files_to_modify)

    lines = [
        '"""Test file for Issue #{issue_number}.',
        "",
        "Generated by AgentOS TDD Testing Workflow.",
        "Tests will fail with ImportError until implementation exists (TDD RED phase).",
        '"""',
        "",
        "import pytest",
        "",
    ]

    # Add implementation module import - this is the TDD RED trigger
    if impl_module:
        lines.extend([
            "# TDD: This import fails until implementation exists (RED phase)",
            f"# Once implemented, tests can run (GREEN phase)",
            f"from {impl_module} import *  # noqa: F401, F403",
            "",
        ])
    lines.append("")

    # Add fixtures if needed
    if any(s.get("mock_needed") for s in scenarios):
        lines.extend([
            "# Fixtures for mocking",
            "@pytest.fixture",
            "def mock_external_service():",
            '    """Mock external service for isolation."""',
            "    # TODO: Implement mock",
            "    yield None",
            "",
            "",
        ])

    if integration_tests or e2e_tests:
        lines.extend([
            "# Integration/E2E fixtures",
            "@pytest.fixture",
            "def test_client():",
            '    """Test client for API calls."""',
            "    # TODO: Implement test client",
            "    yield None",
            "",
            "",
        ])

    # Generate unit tests
    if unit_tests:
        lines.append("# Unit Tests")
        lines.append("# -----------")
        lines.append("")

        for scenario in unit_tests:
            lines.extend(_generate_test_function(scenario, issue_number))

    # Generate integration tests
    if integration_tests:
        lines.append("")
        lines.append("# Integration Tests")
        lines.append("# -----------------")
        lines.append("")

        for scenario in integration_tests:
            lines.extend(_generate_test_function(scenario, issue_number, fixture="test_client"))

    # Generate E2E tests
    if e2e_tests:
        lines.append("")
        lines.append("# E2E Tests")
        lines.append("# ---------")
        lines.append("")

        for scenario in e2e_tests:
            lines.extend(_generate_test_function(scenario, issue_number, fixture="test_client"))

    # Generate other tests
    if other_tests:
        lines.append("")
        lines.append("# Other Tests")
        lines.append("# -----------")
        lines.append("")

        for scenario in other_tests:
            lines.extend(_generate_test_function(scenario, issue_number))

    # Format issue_number in docstring
    content = "\n".join(lines)
    content = content.replace("{issue_number}", str(issue_number))

    return content


def _generate_test_function(
    scenario: TestScenario,
    issue_number: int,
    fixture: str | None = None,
) -> list[str]:
    """Generate a single test function.

    Args:
        scenario: Test scenario.
        issue_number: GitHub issue number.
        fixture: Optional fixture to include in function signature.

    Returns:
        Lines of the test function.
    """
    name = scenario.get("name", "test_unnamed")
    # Ensure name starts with test_
    if not name.startswith("test_"):
        name = f"test_{name}"

    # Clean up name to be a valid Python identifier
    name = re.sub(r"[^a-zA-Z0-9_]", "_", name)
    name = re.sub(r"_+", "_", name)  # Remove duplicate underscores

    description = scenario.get("description", "")
    requirement_ref = scenario.get("requirement_ref", "")
    assertions = scenario.get("assertions", [])
    mock_needed = scenario.get("mock_needed", False)
    test_type = scenario.get("test_type", "unit").lower()

    lines = []

    # Add pytest markers for non-unit test types
    # This enables e2e_validation.py to filter with '-m e2e or integration'
    if test_type == "e2e":
        lines.append("@pytest.mark.e2e")
    elif test_type == "integration":
        lines.append("@pytest.mark.integration")

    # Function signature
    if fixture:
        if mock_needed:
            lines.append(f"def {name}({fixture}, mock_external_service):")
        else:
            lines.append(f"def {name}({fixture}):")
    elif mock_needed:
        lines.append(f"def {name}(mock_external_service):")
    else:
        lines.append(f"def {name}():")

    # Docstring
    docstring_lines = [f'    """']
    if description:
        # Wrap description at 70 chars
        wrapped = _wrap_text(description, 70)
        for line in wrapped:
            docstring_lines.append(f"    {line}")
    else:
        docstring_lines.append(f"    Test: {name}")

    if requirement_ref:
        docstring_lines.append(f"")
        docstring_lines.append(f"    Requirement: {requirement_ref}")

    if assertions:
        docstring_lines.append(f"")
        docstring_lines.append(f"    Assertions:")
        for assertion in assertions[:3]:  # Limit to 3
            docstring_lines.append(f"    - {assertion}")

    docstring_lines.append(f'    """')
    lines.extend(docstring_lines)

    # Test body - TDD style
    # The import at the top of the file will fail until implementation exists
    # Once implemented, these assertions will run
    lines.append("    # TDD: Arrange")
    lines.append("    # Set up test data")
    lines.append("")
    lines.append("    # TDD: Act")
    lines.append("    # Call the function under test")
    lines.append("")
    lines.append("    # TDD: Assert")

    # Convert assertion descriptions to actual assertions
    if assertions:
        for i, assertion in enumerate(assertions[:3]):
            # Generate a real assertion based on the description
            lines.append(f"    # {assertion}")
            lines.append(f"    assert True, 'Placeholder for: {assertion}'")
        lines.append("")
    else:
        # Default assertion if no specific ones provided
        lines.append(f"    # Verify {name} works correctly")
        lines.append(f"    assert True, '{name} executed successfully'")
        lines.append("")

    lines.append("")

    return lines


def _wrap_text(text: str, width: int) -> list[str]:
    """Wrap text at specified width."""
    words = text.split()
    lines = []
    current_line = []
    current_length = 0

    for word in words:
        if current_length + len(word) + 1 <= width:
            current_line.append(word)
            current_length += len(word) + 1
        else:
            if current_line:
                lines.append(" ".join(current_line))
            current_line = [word]
            current_length = len(word)

    if current_line:
        lines.append(" ".join(current_line))

    return lines if lines else [""]


def determine_test_file_path(
    issue_number: int,
    scenarios: list[TestScenario],
    repo_root: Path,
) -> Path:
    """Determine the appropriate path for the test file.

    Args:
        issue_number: GitHub issue number.
        scenarios: List of test scenarios.
        repo_root: Repository root path.

    Returns:
        Path for the test file.
    """
    # Default test directory
    tests_dir = repo_root / "tests"
    tests_dir.mkdir(parents=True, exist_ok=True)

    # Extract module name from scenarios if possible
    # For now, use issue number as identifier
    return tests_dir / f"test_issue_{issue_number}.py"


def scaffold_tests(state: TestingWorkflowState) -> dict[str, Any]:
    """N2: Generate executable test stubs.

    Args:
        state: Current workflow state.

    Returns:
        State updates with test file paths.
    """
    print("\n[N2] Scaffolding tests...")

    # Check for mock mode
    if state.get("mock_mode"):
        return _mock_scaffold_tests(state)

    # Get data from state
    issue_number = state.get("issue_number", 0)
    test_scenarios = state.get("test_scenarios", [])
    files_to_modify = state.get("files_to_modify", [])
    repo_root_str = state.get("repo_root", "")
    repo_root = Path(repo_root_str) if repo_root_str else get_repo_root()

    # --------------------------------------------------------------------------
    # GUARD: Validate scenarios
    # --------------------------------------------------------------------------
    if not test_scenarios:
        print("    [GUARD] BLOCKED: No test scenarios to scaffold")
        return {
            "error_message": "GUARD: No test scenarios available",
        }
    # --------------------------------------------------------------------------

    print(f"    Scaffolding {len(test_scenarios)} test scenarios")

    # Determine test file path
    test_file_path = determine_test_file_path(issue_number, test_scenarios, repo_root)
    print(f"    Test file: {test_file_path}")

    # Generate test file content
    module_name = f"issue_{issue_number}"
    content = generate_test_file_content(
        test_scenarios, module_name, issue_number, files_to_modify
    )

    # Write test file
    test_file_path.write_text(content, encoding="utf-8")
    print(f"    Generated {len(test_scenarios)} tests")

    # Save to audit trail
    audit_dir = Path(state.get("audit_dir", ""))
    if audit_dir.exists():
        file_num = next_file_number(audit_dir)
        save_audit_file(audit_dir, file_num, "test-scaffold.py", content)
    else:
        file_num = state.get("file_counter", 0)

    # Log scaffolding
    log_workflow_execution(
        target_repo=repo_root,
        issue_number=issue_number,
        workflow_type="testing",
        event="tests_scaffolded",
        details={
            "test_file": str(test_file_path),
            "test_count": len(test_scenarios),
        },
    )

    return {
        "test_files": [str(test_file_path)],
        "file_counter": file_num,
        "error_message": "",
    }


def _mock_scaffold_tests(state: TestingWorkflowState) -> dict[str, Any]:
    """Mock implementation for testing."""
    issue_number = state.get("issue_number", 42)
    test_scenarios = state.get("test_scenarios", [])
    repo_root_str = state.get("repo_root", "")
    repo_root = Path(repo_root_str) if repo_root_str else get_repo_root()

    # Generate actual test file for mock mode too
    test_file_path = determine_test_file_path(issue_number, test_scenarios, repo_root)

    if test_scenarios:
        content = generate_test_file_content(test_scenarios, f"issue_{issue_number}", issue_number)
    else:
        content = '''"""Mock test file for testing."""

import pytest


def test_mock_example():
    """Mock test that will fail."""
    assert False, "TDD: Implementation pending for test_mock_example"
'''

    test_file_path.write_text(content, encoding="utf-8")

    # Save to audit
    audit_dir = Path(state.get("audit_dir", ""))
    if audit_dir.exists():
        file_num = next_file_number(audit_dir)
        save_audit_file(audit_dir, file_num, "test-scaffold.py", content)
    else:
        file_num = state.get("file_counter", 0)

    print(f"    [MOCK] Scaffolded tests to {test_file_path}")

    return {
        "test_files": [str(test_file_path)],
        "file_counter": file_num,
        "error_message": "",
    }
